{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし、DMD (Dynamic Mode Decomposition) ってなんだ？\n",
    "\n",
    "※ここで取り扱う全ての行列は、要素が実数です。複素数にも適用できますが、ここでは説明しません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前知識"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (L2 / フロベニウス)ノルム\n",
    "\n",
    "ベクトルや行列の大きさを調べる指標。言葉で言うと「要素の二乗和の平方」。式で言うと、\n",
    "\n",
    "- L2ノルム: $X \\in \\R^N$ について、 $\\| X \\|_2 = \\sqrt{\\sum_{n=0}^N x_n^2}$\n",
    "- フロベニウスノルム: $X \\in \\R^{M \\times N}$ について、$\\| X \\|_F = \\sqrt{\\sum_{m=0}^M\\sum_{n=0}^N x_{mn}^2}$\n",
    "\n",
    "二つは対象が違うだけで言っていることは同じである。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2ノルム\n",
    "x_array = np.array([1, -1, 2])\n",
    "\n",
    "# 手作業\n",
    "print(np.sqrt(np.sum(x_array**2)))\n",
    "# 組み込み関数\n",
    "print(np.linalg.norm(x_array))\n",
    "\n",
    "# フロベニウスノルム\n",
    "x_mat = np.array(\n",
    "  [\n",
    "    [0, -1, 0],\n",
    "    [1, 0, 2],\n",
    "    [0, -1, 0]\n",
    "  ]\n",
    ")\n",
    "\n",
    "# 手作業\n",
    "print(np.sqrt(np.sum(x_mat**2)))\n",
    "# 組み込み関数\n",
    "print(np.linalg.norm(x_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特異値分解 (SVD分解)\n",
    "\n",
    "ある任意の行列を $A = U\\Sigma V$ の形に分解すること。ただし、\n",
    "- UやVは正方行列かつ直交行列 ($UU^T=I, VV^T=I$)\n",
    "- $\\Sigma$ は左上から右下に向かって特異値を並べた行列(他位置は0)であり、サイズは $A$ に等しい\n",
    "\n",
    "$AA^T$ か $A^TA$ の固有値分解ができたならば、特異値分解を計算できたことになる。($U$と$V$は対応しているため、片方が決まったら残りは固有値分解でない方法で解く必要がある)\n",
    "- $A^TA = (U\\Sigma V)^T (U\\Sigma V) = V^T\\Sigma^T U^T (U\\Sigma V) = V^T\\Sigma^T\\Sigma V$ より、$(A^TA)V^T = V^T(\\Sigma^T\\Sigma) $。つまり、固有値行列=$\\Sigma^T\\Sigma$、固有ベクトル行列=$V^T$\n",
    "- $AA^T = (U\\Sigma V)(U\\Sigma V)^T = (U\\Sigma V)V^T\\Sigma^T U^T = U\\Sigma\\Sigma^TU^T$ より、$(AA^T)U = U(\\Sigma\\Sigma^T) $。つまり、固有値行列=$\\Sigma\\Sigma^T$、固有ベクトル行列=$U$\n",
    "\n",
    "なお、$U, V$の組は複数ありうるが、$\\Sigma$ はただ一つしかない。\n",
    "\n",
    "補足：固有値分解の方法で$U$か$V$が求まったら、残りは$A = U\\Sigma V$に立ち返って求める。今回はVが求まったとして説明。\n",
    "\n",
    "> $A = U\\Sigma V$  \n",
    "> $AV^T = U\\Sigma $\n",
    "\n",
    "と変形できるので、結局\n",
    "\n",
    "> $U = AV^T\\Sigma^{-1} $\n",
    "\n",
    "となる。ここで、\n",
    "- $\\Sigma^{-1}$ は $\\Sigma$ の各非ゼロ要素の逆数を取り、転置したもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD分解\n",
    "# 入力\n",
    "a = np.random.randn(3, 5)\n",
    "print(\"a:\")\n",
    "print(a)\n",
    "\n",
    "# 手作業で計算してみる\n",
    "sigma_sq, v_t = np.linalg.eig(a.T @ a)\n",
    "sort_index = np.argsort(-np.abs(sigma_sq))\n",
    "sigma_sq = sigma_sq[sort_index]\n",
    "v_t = v_t[:, sort_index]\n",
    "# 正しく分解できたかどうか\n",
    "print(np.linalg.norm(a.T @ a @ v_t - v_t @ np.diag(sigma_sq)))\n",
    "\n",
    "v = v_t.T\n",
    "sigma = np.zeros_like(a, dtype=float)\n",
    "np.fill_diagonal(sigma, np.sqrt(sigma_sq))\n",
    "\n",
    "u = a @ v_t @ np.where(sigma != 0, 1 / sigma, 0).T\n",
    "\n",
    "print(\"\\nu:\")\n",
    "print(u)\n",
    "\n",
    "print(\"\\nsigma:\")\n",
    "print(sigma)\n",
    "\n",
    "print(\"\\nv:\")\n",
    "print(v)\n",
    "\n",
    "print(\"\\nu sigma v (= a):\")\n",
    "print(u @ sigma @ v)\n",
    "\n",
    "# 組み込み関数\n",
    "u, s, v = np.linalg.svd(a)\n",
    "sigma = np.zeros_like(a, dtype=float)\n",
    "np.fill_diagonal(sigma, s)\n",
    "\n",
    "print(\"\\nu:\")\n",
    "print(u)\n",
    "\n",
    "print(\"\\nsigma:\")\n",
    "print(sigma)\n",
    "\n",
    "print(\"\\nv:\")\n",
    "print(v)\n",
    "\n",
    "print(\"\\nu sigma v (= a):\")\n",
    "print(u @ sigma @ v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ムーア・ペンローズの疑似逆行列\n",
    "\n",
    "逆行列は正方行列であってかつ正則でなければ定義されない。何とかそんな行列以外でも逆行列のようなものを定義したい。  \n",
    "そのような時に使えるのが疑似逆行列。\n",
    "\n",
    "ある $m \\times n$ 行列$A \\in \\R^{m \\times n}$について、以下を満たす$n \\times m$行列 $A^+ \\in \\R^{n \\times m}$ がただ一つ存在し、$A^+$ を疑似逆行列と呼ぶ。\n",
    "\n",
    "> $A^+AA^+ = A^+$   \n",
    "> $AA^+A = A$  \n",
    "> $(A^+A)^T = (A^+A)$  \n",
    "> $(AA^+)^T = (AA^+)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "疑似逆行列は、特異値分解を用いて求めることができる。\n",
    "\n",
    "$A = U\\Sigma V$ より、\n",
    "\n",
    "> $A^+ = V^T \\Sigma^{-1} U^T$\n",
    "\n",
    "で求められる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(3, 5)\n",
    "print(\"a:\")\n",
    "print(a)\n",
    "\n",
    "u, s, v = np.linalg.svd(a)\n",
    "sigma = np.zeros_like(a, dtype=float)\n",
    "np.fill_diagonal(sigma, s)\n",
    "\n",
    "# ムーア・ペンローズの逆行列\n",
    "a_inv = v.T @ np.where(sigma != 0, 1 / sigma, 0).T @ u.T\n",
    "\n",
    "print(\"\\na_inv:\")\n",
    "print(a_inv)\n",
    "\n",
    "print(np.allclose(a_inv @ a @ a_inv, a_inv))\n",
    "print(np.allclose(a @ a_inv @ a, a))\n",
    "print(np.allclose((a_inv @ a).T, a_inv @ a))\n",
    "print(np.allclose((a @ a_inv).T, a @ a_inv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMDが目指すところ\n",
    "\n",
    "DMD は実際のデータから時空間パターンを抽出する事を目指す。\n",
    "\n",
    "**ここでは実数の範囲で考えることにする。** (本来は複素数の範囲で適用可能)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、以下のデータ (スナップショットと呼ばれる) からパターンを抽出することを考える\n",
    "\n",
    "> $X = \\{x_1, ..., x_m | x \\in \\R^n\\}$\n",
    "\n",
    "ただし、\n",
    "- $m$ はデータポイントの数\n",
    "- $n$ は各データポイントのベクトルの要素数\n",
    "\n",
    "要するに、$X$は $X \\in \\R^{n \\times m}$ の行列である。\n",
    "\n",
    "普通、$n >> m$ とすることが多い (小さめなスナップショットを取る)。が、別にこれに限らなくともよい。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DMDの目標は、以下のように次のデータポイントを予測する行列 $A \\in \\R^{n \\times n}$を見つけることである**\n",
    "\n",
    "> $x_{j+1} \\approx A x_{j}$\n",
    "\n",
    "ただし、\n",
    "- $j$ は整数 $(1 \\le j \\le m - 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$ を求めるためには、一般に次の最適化問題を解く\n",
    "\n",
    "> $\\underset{A \\in \\R^{n \\times n}}{min} \\sum_{j=1}^{m-1} \\|A x_j - x_{j+1}\\|^2_2$\n",
    "\n",
    "ただし、\n",
    "- $\\| ・ \\|^2_2$ はL2ノルムの二乗 (つまり、要素の二乗和)\n",
    "\n",
    "が、非常に面倒(計算コストがかかる)なので通常は近似して計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概略した方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適化問題を各$j$ について計算するのは面倒なので、行列にまとめる。\n",
    "$x_j$ の集合を $X_1$とし、これを右に一つずらしたもの (つまり、$x_{j+1}$ の集合) を$X_2$ とする。つまり、\n",
    "\n",
    "> $X_1 = \\{x_1, x_2, ..., x_{m-1}\\}$  \n",
    "> $X_2 = \\{x_2, x_3, ..., x_{m}\\}$\n",
    "\n",
    "とすると、問題は次のように書き直せる。\n",
    "\n",
    "> $\\underset{A \\in \\R^{n \\times n}}{min} \\|A X_1 - X_2\\|^2_F$\n",
    "\n",
    "ただし、\n",
    "- $\\| ・ \\|^2_F$ はフロベニウスノルムの二乗 (つまり、要素の二乗和)\n",
    "\n",
    "$X_1$ と $X_2$ は、ずらしただけなのでサイズが一致する $( X_1, X_2 \\in \\R^{n \\times (m-1)})$。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "問題を端的に $AX_1 - X_2 \\approx O$ と思い直すことにすると、$A$ は下式を計算すればよいことになる (厳密な証明はしない)\n",
    "\n",
    "> $A = X_2 X_1^+$\n",
    "\n",
    "ここで、$X_1^+$ は ムーア・ペンローズの疑似逆行列 である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、例えば $k+1$ ステップ目のデータを計算するには、\n",
    "\n",
    ">$x_{k+1} \\approx A^{k}x_1$\n",
    "\n",
    "のように計算できる。だが、$A^k$を計算するのは面倒。  \n",
    "\n",
    "ここで、Aを対角化可能だと仮定することで計算を簡単にできる。つまり、$A$ を以下の式のように対角化する：\n",
    "> $A = V\\Lambda V^{-1}$\n",
    "\n",
    "ただし、\n",
    "- $V$ は対角化行列\n",
    "  - 対角化行列であるということは、固有ベクトルを集めた行列でもあるということ\n",
    "- $\\Lambda$ は固有値を対角線上に持つ正方行列\n",
    "\n",
    "こうすることで、$x_{k+1}$ は以下で表せる。  \n",
    "> $x_{k+1} \\approx A^{k}x_1 = (V\\Lambda V^{-1})^k x_1 = V\\Lambda^k V^{-1} x_1 = V\\Lambda^k x_0$\n",
    "\n",
    "ただし、\n",
    "- $x_0 = V^{-1} x_1$ とおいた。$x_0$ は初項のようなものである。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまで発展させると、DMDの結果は $(\\Lambda, V, x_0)$ を決定することに対応するといえる。それぞれ、DMD固有値、DMDモード、DMD振幅 と解釈される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD による決定方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この問題を(近似的に)解く方法はいくつか提案されているが、ここではSVD分解を用いた方法を説明する。\n",
    "\n",
    "データ $X$ のスライス $X_1$ が \n",
    "> $X_1=U\\Sigma V$   \n",
    "\n",
    "とSVD分解できるとする。このとき、それぞれの次元は、\n",
    "- $X_1 \\in \\R^{n \\times (m-1)}$\n",
    "- $U \\in \\R^{n \\times r}$  \n",
    "- $\\Sigma \\in \\R^{r \\times r}$\n",
    "- $V \\in \\R^{r \\times m}$\n",
    "\n",
    "となる。(rは自由に決め、SVD分解の結果をトリミングして決定する。これにより計算が簡略化する)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、次のような$S$を決めると計算が簡単になる、というのがSVDによる方法である。\n",
    "\n",
    "> $S= U^*AU \\in \\R^{r \\times r}$\n",
    "\n",
    "$S$を詳しく計算していくと、\n",
    "\n",
    "> $S = U^*AU = U^*AU \\Sigma V V^* \\Sigma^{-1} = U^*A X_1 V^* \\Sigma^{-1} = U^* X_2 V^* \\Sigma^{-1}$\n",
    "\n",
    "$S$ と $A$ の固有値は一致することが知られており、また、$S$ の固有ベクトルを$V'$ とすると、$V=UV$ として$A$の固有ベクトルを求められる (証明略)。したがって、$A$ の計算をせずとも $V$ や $\\Lambda$ を知ることができる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMD \n",
    "r = 5\n",
    "n = 100\n",
    "m = 150\n",
    "\n",
    "x = np.fromfunction(lambda n, m: 90 * np.sin(n * n * n + 3 * m), (n, m))\n",
    "x1 = x[:, :-1]\n",
    "x2 = x[:, 1:]\n",
    "u, sigma, v = np.linalg.svd(x1)\n",
    "\n",
    "u = u[:, :r]\n",
    "sigma = np.diag(sigma[:r])\n",
    "v = v[:r, :]\n",
    "\n",
    "print(\"x1, x2: \",  x1.shape, x2.shape)\n",
    "print(\"u, sigma, v:\", u.shape, sigma.shape, v.shape)\n",
    "\n",
    "s = np.linalg.pinv(u) @ \\\n",
    "    x2 @ \\\n",
    "    np.linalg.pinv(v) @ \\\n",
    "    np.linalg.inv(sigma)\n",
    "\n",
    "lamb, vq = np.linalg.eig(s)\n",
    "v = u @ vq\n",
    "x0 = np.linalg.pinv(v) @ x[:, 0]\n",
    "lamb = np.diag(lamb)\n",
    "\n",
    "print(f\"lambda: {lamb.shape}\")\n",
    "print(f\"v: {v.shape}\")\n",
    "print(f\"x0: {x0.shape}\")\n",
    "\n",
    "# これで、DMDにより \\Lambda, V, X_0 が計算された。再構成誤差を測ってみる\n",
    "def reconstract(k):\n",
    "  return v @ np.where(lamb != 0, np.power(lamb, k), 0.0) @ x0\n",
    "\n",
    "reconstracted = np.array([reconstract(i) for i in range(m)], dtype=float).T\n",
    "print(f\"再構成誤差: {np.linalg.norm(x - reconstracted)}\")\n",
    "\n",
    "for c in range(5):\n",
    "  plt.plot(x[:,c])\n",
    "  plt.plot(reconstracted[:, c])\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献 (Webページは全て6/11に閲覧)\n",
    "\n",
    "1. KRAKE, Tim; WEISKOPF, Daniel; EBERHARDT, Bernhard. Dynamic mode decomposition: Theory and data reconstruction. arXiv preprint arXiv:1909.10466, 2019.\n",
    "2. https://qiita.com/harmegiddo/items/84552c32f4b75c26878a\n",
    "3. https://manabitimes.jp/math/2746"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
